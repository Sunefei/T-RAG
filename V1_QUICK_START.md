# T-RAG V1 å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼ˆæœåŠ¡å™¨è¿è¡Œï¼‰

## âœ… V1 å·²å®Œæˆçš„å·¥ä½œ

### æ–°å¢æ–‡ä»¶
```
T-RAG/src/downstream_inference/
â”œâ”€â”€ adaptive_modules/
â”‚   â”œâ”€â”€ __init__.py              âœ… æ¨¡å—åˆå§‹åŒ–
â”‚   â”œâ”€â”€ prompts.py               âœ… è¡¨æ ¼ä¸“ç”¨ Prompt æ¨¡æ¿
â”‚   â””â”€â”€ query_decomposer.py      âœ… æŸ¥è¯¢åˆ†è§£é€»è¾‘ï¼ˆ300+ è¡Œï¼Œå®Œæ•´å®ç°ï¼‰
â”œâ”€â”€ call_llm_v1.py               âœ… V1 ä¸»ç¨‹åºï¼ˆ400+ è¡Œï¼Œå®Œå…¨å…¼å®¹åŸç‰ˆï¼‰
â”œâ”€â”€ run_v1_comparison.sh         âœ… å¯¹æ¯”å®éªŒè„šæœ¬
â”œâ”€â”€ test_v1_module.py            âœ… æ¨¡å—æµ‹è¯•è„šæœ¬
â””â”€â”€ V1_README.md                 âœ… è¯¦ç»†ä½¿ç”¨æ–‡æ¡£
```

### æ ¸å¿ƒåŠŸèƒ½
1. âœ… æŸ¥è¯¢åˆ†è§£ï¼šå°†å¤æ‚æŸ¥è¯¢æ‹†åˆ†ä¸ºåŸå­çº§å­é—®é¢˜
2. âœ… å¤šè·³æ£€æµ‹ï¼šè‡ªåŠ¨è¯†åˆ«å•è·³/å¤šè·³æŸ¥è¯¢
3. âœ… æ—¥å¿—è®°å½•ï¼šå®Œæ•´çš„åˆ†è§£è¿‡ç¨‹æ—¥å¿—
4. âœ… ç»Ÿè®¡åˆ†æï¼šæŸ¥è¯¢å¤æ‚åº¦ç»Ÿè®¡
5. âœ… å‘åå…¼å®¹ï¼šå¯é€‰å¼€å…³ï¼Œä¸å½±å“åŸæœ‰åŠŸèƒ½

---

## ğŸš€ æœåŠ¡å™¨è¿è¡Œæ­¥éª¤

### æ­¥éª¤ 0ï¼šPush ä»£ç åˆ°æœåŠ¡å™¨

```bash
# åœ¨æœ¬åœ° T-RAG ç›®å½•
cd /Users/sunyifei/Documents/GitHub/T-RAG

# æ·»åŠ æ‰€æœ‰æ–°æ–‡ä»¶
git add src/downstream_inference/adaptive_modules/
git add src/downstream_inference/call_llm_v1.py
git add src/downstream_inference/run_v1_comparison.sh
git add src/downstream_inference/test_v1_module.py
git add src/downstream_inference/V1_README.md
git add V1_QUICK_START.md
git add ADAPTIVE_PLANNING_INTEGRATION.md

# æäº¤
git commit -m "feat: Add V1 query decomposition module

- Implement adaptive_modules with query decomposer
- Add table-specific prompts for decomposition
- Create call_llm_v1.py with decomposition support
- Add comparison script and documentation
- Backward compatible with --use_decomposition flag"

# æ¨é€åˆ°è¿œç¨‹
git push
```

### æ­¥éª¤ 1ï¼šåœ¨æœåŠ¡å™¨ä¸Šé…ç½®

```bash
# SSH åˆ°æœåŠ¡å™¨
ssh your-server

# æ‹‰å–æœ€æ–°ä»£ç 
cd /path/to/T-RAG
git pull

# æ¿€æ´»ç¯å¢ƒ
conda activate trag

# é…ç½® API Key
cd src/downstream_inference
vim key.json
```

åœ¨ `key.json` ä¸­å¡«å…¥ï¼š
```json
{
    "openai": "sk-your-actual-openai-api-key",
    "claude": "<YOUR_CLAUDE_API_KEY>"
}
```

### æ­¥éª¤ 2ï¼šå‡†å¤‡æ•°æ®ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰

```bash
cd /path/to/T-RAG/src/table2graph

# ä¸‹è½½å¹¶å¤„ç†æ•°æ®é›†
bash scripts/prepare_data.sh

# ç­‰å¾…æ•°æ®ä¸‹è½½å®Œæˆ...
# è¿™ä¼šä¸‹è½½ MultiTableQA çš„æ‰€æœ‰æ•°æ®é›†ï¼ˆåŒ…æ‹¬ SQAï¼‰
```

### æ­¥éª¤ 3ï¼šè¿è¡Œæ£€ç´¢æµç¨‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰

```bash
cd /path/to/T-RAG/src/table2graph

# æ­¥éª¤ 3.1: è¡¨æ ¼èšç±»ï¼ˆStage 1&2ï¼‰
bash scripts/table_cluster_run.sh

# æ­¥éª¤ 3.2: å­å›¾æ£€ç´¢ï¼ˆStage 3ï¼‰
python scripts/subgraph_retrieve_run.py

# æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
ls -lh data/sqa/sqa_retrieved_tables_schema_100_50_contriever.jsonl
# åº”è¯¥çœ‹åˆ°è¿™ä¸ªæ–‡ä»¶å­˜åœ¨
```

### æ­¥éª¤ 4ï¼šè¿è¡Œ V1 å¯¹æ¯”å®éªŒ

```bash
cd /path/to/T-RAG/src/downstream_inference

# æ–¹æ³• 1ï¼šä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆæ¨èï¼‰
bash run_v1_comparison.sh

# æ–¹æ³• 2ï¼šæ‰‹åŠ¨è¿è¡Œå¯¹æ¯”å®éªŒ

# å®éªŒ 1ï¼šåŸºçº¿ï¼ˆä¸ä½¿ç”¨åˆ†è§£ï¼‰
python call_llm_v1.py \
    --dataset sqa \
    --topk 50 \
    --model gpt-4o-mini \
    --testing_num 100 \
    --embedding_method contriever

# å®éªŒ 2ï¼šä½¿ç”¨åˆ†è§£ï¼ˆV1 åŠŸèƒ½ï¼‰
python call_llm_v1.py \
    --dataset sqa \
    --topk 50 \
    --model gpt-4o-mini \
    --testing_num 100 \
    --embedding_method contriever \
    --use_decomposition \
    --decomposition_verbose
```

### æ­¥éª¤ 5ï¼šæŸ¥çœ‹ç»“æœ

```bash
cd output/sqa/gpt-4o-mini/

# æŸ¥çœ‹åˆ†è§£ç»Ÿè®¡
cat decomposition_stats_100_50.json

# æŸ¥çœ‹åˆ†è§£æ—¥å¿—ï¼ˆå‰10ä¸ªï¼‰
head -n 10 decomposition_log_100_50.jsonl | python -m json.tool

# æŸ¥çœ‹æ¨ç†ç»“æœ
head -n 5 output_100_50_v1_decomp.jsonl | python -m json.tool
```

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### 1. æ•°æ®é›†è§„æ¨¡é€‰æ‹©

ç”±äºä½ è¦å’Œè®ºæ–‡å¯¹æ¯”ï¼Œéœ€è¦ä½¿ç”¨**å®Œæ•´æµ‹è¯•é›†**ï¼š

**ä¿®æ”¹æµ‹è¯•é›†å¤§å°çš„ä½ç½®**ï¼š

```bash
# åœ¨ subgraph_retrieve_run.py ä¸­ä¿®æ”¹
testing_num = 100  # æ”¹ä¸ºä½ éœ€è¦çš„æ•°é‡

# æˆ–è€…åœ¨è¿è¡Œæ—¶ä¿®æ”¹ï¼š
cd table2graph
vim scripts/subgraph_retrieve_run.py
# æ‰¾åˆ° testing_num = 100ï¼Œæ”¹ä¸ºä½ æƒ³è¦çš„æ•°é‡
```

**å„æ•°æ®é›†çš„å®Œæ•´è§„æ¨¡**ï¼š
- SQA: ~8,000+ æµ‹è¯•æ ·æœ¬
- HybridQA: ~2,000+ æµ‹è¯•æ ·æœ¬
- WTQ: ~4,000+ æµ‹è¯•æ ·æœ¬
- TabFact: ~12,000+ æµ‹è¯•æ ·æœ¬

**å»ºè®®**ï¼š
- å…ˆç”¨ `testing_num=100` å¿«é€ŸéªŒè¯ V1 åŠŸèƒ½æ­£å¸¸
- ç¡®è®¤æ— è¯¯åï¼Œå†è¿è¡Œå®Œæ•´æ•°æ®é›†

### 2. API æˆæœ¬ä¼°ç®—

ä½¿ç”¨ GPT-4o-mini çš„æˆæœ¬ï¼š
- æ¯ä¸ªæŸ¥è¯¢ â‰ˆ 2 æ¬¡ LLM è°ƒç”¨ï¼ˆåŸå§‹æ¨ç† + åˆ†è§£ï¼‰
- 100 ä¸ªæ ·æœ¬ â‰ˆ $0.50 - $1.00
- 8,000 ä¸ªæ ·æœ¬ï¼ˆå®Œæ•´ SQAï¼‰â‰ˆ $40 - $80

**å»ºè®®**ï¼š
1. å…ˆç”¨å°è§„æ¨¡ï¼ˆ100ä¸ªï¼‰éªŒè¯
2. ç¡®è®¤æ•ˆæœåå†è·‘å®Œæ•´æ•°æ®é›†
3. è€ƒè™‘ä½¿ç”¨ `gpt-4o-mini`ï¼ˆæ›´ä¾¿å®œï¼‰è€Œé `gpt-4o`

### 3. æ–‡ä»¶è·¯å¾„æ£€æŸ¥

**ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨**ï¼š
```bash
# æ£€ç´¢ç»“æœæ–‡ä»¶ï¼ˆç”± table2graph ç”Ÿæˆï¼‰
../table2graph/data/sqa/sqa_retrieved_tables_schema_100_50_contriever.jsonl
```

å¦‚æœä¸å­˜åœ¨ï¼Œè¿è¡Œä¼šæŠ¥é”™ï¼š
```
âŒ ERROR: Retrieved tables file not found!
```

**è§£å†³æ–¹æ³•**ï¼šå…ˆè¿è¡Œæ­¥éª¤ 3 çš„æ£€ç´¢æµç¨‹ã€‚

### 4. è¾“å‡ºæ–‡ä»¶å‘½å

V1 çš„è¾“å‡ºæ–‡ä»¶æœ‰ç‰¹æ®Šåç¼€ï¼š
- åŸºçº¿ï¼ˆæ— åˆ†è§£ï¼‰ï¼š`output_100_50_v1_baseline.jsonl`
- åˆ†è§£ç‰ˆæœ¬ï¼š`output_100_50_v1_decomp.jsonl`

**è¿™æ ·å¯ä»¥é¿å…è¦†ç›–åŸå§‹ T-RAG çš„ç»“æœ**ï¼Œæ–¹ä¾¿å¯¹æ¯”ã€‚

### 5. æ¨¡å—å¯¼å…¥é—®é¢˜

å¦‚æœé‡åˆ°å¯¼å…¥é”™è¯¯ï¼š
```python
ModuleNotFoundError: No module named 'adaptive_modules'
```

**è§£å†³æ–¹æ³•**ï¼šç¡®ä¿åœ¨ `src/downstream_inference` ç›®å½•ä¸‹è¿è¡Œï¼š
```bash
cd /path/to/T-RAG/src/downstream_inference
python call_llm_v1.py ...
```

---

## ğŸ“Š é¢„æœŸè¾“å‡ºç¤ºä¾‹

### æˆåŠŸè¿è¡Œåä¼šçœ‹åˆ°ï¼š

#### 1. æ§åˆ¶å°è¾“å‡º
```
======================================================================
T-RAG with Adaptive Planning - Version 1
======================================================================
Dataset: sqa
Model: gpt-4o-mini
Top-K: 50
Testing samples: 100
Embedding method: contriever
V1 Decomposition: ENABLED
======================================================================

Loading retrieved tables from: ../table2graph/data/sqa/sqa_retrieved_tables_schema_100_50_contriever.jsonl
Loaded 100 instances

Processing queries...
Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:23<00:00,  3.23s/it]

Saving results to: output/sqa/gpt-4o-mini/output_100_50_v1_decomp.jsonl
Saving decomposition log to: output/sqa/gpt-4o-mini/decomposition_log_100_50.jsonl

======================================================================
V1 DECOMPOSITION STATISTICS
======================================================================
Total queries: 100
Multi-hop queries: 45 (45.0%)
Single-hop queries: 55 (55.0%)
Average requirements per query: 1.65
Decomposition failures: 0
======================================================================

âœ… Inference complete!

Next step: Run evaluation
  python evaluation.py --dataset sqa --model gpt-4o-mini --topk 50 --testing_num 100
```

#### 2. åˆ†è§£æ—¥å¿—ç¤ºä¾‹
```bash
cat decomposition_log_100_50.jsonl | head -n 1 | python -m json.tool
```

è¾“å‡ºï¼š
```json
{
  "query_idx": 0,
  "query": "What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?",
  "decomposition": {
    "user_goal": "Find government position of actress",
    "requirements": [
      {
        "requirement_id": "req1",
        "question": "Who portrayed Corliss Archer in the film Kiss and Tell?",
        "depends_on": null
      },
      {
        "requirement_id": "req2",
        "question": "What government position was held by [answer from req1]?",
        "depends_on": "req1"
      }
    ]
  },
  "is_multi_hop": true,
  "num_requirements": 2
}
```

#### 3. ç»Ÿè®¡æ–‡ä»¶
```bash
cat decomposition_stats_100_50.json
```

è¾“å‡ºï¼š
```json
{
  "total_queries": 100,
  "multi_hop_queries": 45,
  "single_hop_queries": 55,
  "avg_requirements": 1.65,
  "decomposition_failures": 0
}
```

---

## ğŸ” å¦‚ä½•éªŒè¯ V1 æ­£ç¡®è¿è¡Œ

### æ£€æŸ¥æ¸…å•ï¼š

1. âœ… **åˆ†è§£æ—¥å¿—æ–‡ä»¶å­˜åœ¨**
   ```bash
   ls -lh output/sqa/gpt-4o-mini/decomposition_log_100_50.jsonl
   # åº”è¯¥æœ‰å†…å®¹ï¼ˆä¸æ˜¯ç©ºæ–‡ä»¶ï¼‰
   ```

2. âœ… **åˆ†è§£ç»Ÿè®¡æ­£å¸¸**
   ```bash
   cat output/sqa/gpt-4o-mini/decomposition_stats_100_50.json
   # åº”è¯¥çœ‹åˆ° multi_hop_queries > 0
   ```

3. âœ… **è¾“å‡ºæ–‡ä»¶åŒ…å«åˆ†è§£ä¿¡æ¯**
   ```bash
   head -n 1 output/sqa/gpt-4o-mini/output_100_50_v1_decomp.jsonl | python -m json.tool
   # åº”è¯¥çœ‹åˆ° "decomposition" å­—æ®µ
   ```

4. âœ… **æ— é”™è¯¯ä¿¡æ¯**
   ```bash
   # æ§åˆ¶å°ä¸åº”è¯¥æœ‰å¤§é‡ ERROR æˆ– WARNING
   # å…è®¸ä¸ªåˆ«æŸ¥è¯¢çš„ warningï¼Œä½†ä¸åº”è¯¥å…¨éƒ¨å¤±è´¥
   ```

---

## ğŸ› å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1ï¼šJSON è§£æå¤±è´¥
```
[WARNING] Decomposition failed for query X: Expecting value: line 1 column 1 (char 0)
```

**å¯èƒ½åŸå› **ï¼š
- LLM æ²¡æœ‰è¿”å› JSON
- API è°ƒç”¨å¤±è´¥

**è§£å†³æ–¹æ³•**ï¼š
- æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®
- æŸ¥çœ‹ LLM è¿”å›çš„åŸå§‹æ–‡æœ¬ï¼ˆæ‰“å¼€ `--decomposition_verbose`ï¼‰
- å¯èƒ½éœ€è¦è°ƒæ•´ promptï¼ˆåœ¨ `adaptive_modules/prompts.py`ï¼‰

### é—®é¢˜ 2ï¼šæ‰€æœ‰æŸ¥è¯¢éƒ½æ˜¯å•è·³
```
Multi-hop queries: 0 (0.0%)
```

**å¯èƒ½åŸå› **ï¼š
- Prompt ä¸å¤Ÿæ˜ç¡®
- æ•°æ®é›†æœ¬èº«ç¡®å®æ˜¯å•è·³ä¸ºä¸»
- LLM æ²¡æœ‰ç†è§£åˆ†è§£ä»»åŠ¡

**è§£å†³æ–¹æ³•**ï¼š
- æ‰‹åŠ¨æ£€æŸ¥å‡ ä¸ªåˆ†è§£ç»“æœï¼Œçœ‹æ˜¯å¦åˆç†
- å¦‚æœæ˜¯ SQA æ•°æ®é›†ï¼Œåº”è¯¥æœ‰ä¸€å®šæ¯”ä¾‹çš„å¤šè·³æŸ¥è¯¢

### é—®é¢˜ 3ï¼šåˆ†è§£æ—¥å¿—ä¸ºç©º
```
decomposition_log_100_50.jsonl æ–‡ä»¶ä¸å­˜åœ¨
```

**åŸå› **ï¼šæ²¡æœ‰å¯ç”¨ `--use_decomposition`

**è§£å†³æ–¹æ³•**ï¼š
```bash
# ç¡®ä¿æ·»åŠ è¿™ä¸ªå‚æ•°
python call_llm_v1.py ... --use_decomposition
```

---

## ğŸ“ˆ V1 çš„è¯„ä¼°å’Œå¯¹æ¯”

### è¿è¡Œè¯„ä¼°

```bash
cd /path/to/T-RAG/src/downstream_inference

# è¯„ä¼°åŸºçº¿
python evaluation.py \
    --dataset sqa \
    --model gpt-4o-mini \
    --topk 50 \
    --testing_num 100

# è¯„ä¼°åˆ†è§£ç‰ˆæœ¬ï¼ˆç›¸åŒå‘½ä»¤ï¼Œä¼šè‡ªåŠ¨æ‰¾å¯¹åº”çš„è¾“å‡ºæ–‡ä»¶ï¼‰
python evaluation.py \
    --dataset sqa \
    --model gpt-4o-mini \
    --topk 50 \
    --testing_num 100
```

### V1 é¢„æœŸç»“æœ

**é‡è¦**ï¼šV1 é˜¶æ®µçš„ EM/F1 åº”è¯¥ä¸åŸºçº¿**åŸºæœ¬ç›¸åŒ**ã€‚

ä¸ºä»€ä¹ˆï¼Ÿ
- V1 åªåšåˆ†è§£å’Œè®°å½•ï¼Œ**ä¸æ”¹å˜æ¨ç†æµç¨‹**
- å®é™…æ¨ç†ä»ç„¶æ˜¯åŸå§‹ T-RAG çš„å•æ¬¡æ¨ç†
- åˆ†è§£ä¿¡æ¯ä»…ç”¨äºæ—¥å¿—å’Œåˆ†æ

**V1 çš„ç›®æ ‡**ï¼š
1. âœ… éªŒè¯åˆ†è§£é€»è¾‘æ­£ç¡®
2. âœ… åˆ†ææ•°æ®é›†ä¸­å¤šè·³æŸ¥è¯¢çš„æ¯”ä¾‹
3. âœ… ä¸º V2/V3/V4 æ‰“å¥½åŸºç¡€

**çœŸæ­£çš„æå‡ä¼šåœ¨**ï¼š
- V2ï¼šäº‹å®æå–è¯„ä¼°ï¼ˆ+1-2% EMï¼‰
- V3ï¼šé‡è§„åˆ’èƒ½åŠ›ï¼ˆ+2-3% EMï¼‰
- V4ï¼šå®Œæ•´è¿­ä»£å¾ªç¯ï¼ˆ+3-5% EMï¼‰

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### å®Œæˆ V1 åï¼š

1. **æ£€æŸ¥åˆ†è§£è´¨é‡**
   ```bash
   # éšæœºæŸ¥çœ‹ 10 ä¸ªåˆ†è§£ç»“æœ
   shuf -n 10 decomposition_log_100_50.jsonl | python -m json.tool
   ```

2. **åˆ†æå¤šè·³æŸ¥è¯¢**
   ```python
   import json

   with open('decomposition_log_100_50.jsonl') as f:
       logs = [json.loads(line) for line in f]

   multi_hop = [log for log in logs if log['is_multi_hop']]
   print(f"Multi-hop: {len(multi_hop)}/{len(logs)}")

   # æŸ¥çœ‹å‡ ä¸ªä¾‹å­
   for log in multi_hop[:3]:
       print(f"\nQuery: {log['query']}")
       for req in log['decomposition']['requirements']:
           print(f"  {req['requirement_id']}: {req['question']}")
   ```

3. **å‡†å¤‡ V2 å¼€å‘**
   - å¦‚æœåˆ†è§£è´¨é‡å¥½ â†’ å¯ä»¥å¼€å§‹ V2ï¼ˆäº‹å®æå–ï¼‰
   - å¦‚æœåˆ†è§£è´¨é‡å·® â†’ éœ€è¦è°ƒä¼˜ Prompt

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œæ£€æŸ¥ï¼š

1. **æ—¥å¿—æ–‡ä»¶**ï¼š`logs/sqa/*.log`
2. **é”™è¯¯è¾“å‡º**ï¼šæ§åˆ¶å°çš„ ERROR/WARNING ä¿¡æ¯
3. **API è°ƒç”¨**ï¼šæ˜¯å¦æœ‰ rate limit æˆ– quota é”™è¯¯

**å‡†å¤‡å¥½ç»§ç»­ V2 äº†å—ï¼Ÿ** å‘Šè¯‰æˆ‘ V1 çš„è¿è¡Œç»“æœï¼Œæˆ‘ä¼šå¸®ä½ å¼€å§‹ V2 çš„å¼€å‘ï¼
