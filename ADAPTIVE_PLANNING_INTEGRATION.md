# T-RAG + REAP Adaptive Planning å¢é‡å¼é›†æˆæŒ‡å—

## ğŸ“‹ ç›®å½•
1. [T-RAG è¿è¡ŒæŒ‡å—](#1-t-rag-è¿è¡ŒæŒ‡å—)
2. [å¢é‡å¼å¼€å‘è®¡åˆ’](#2-å¢é‡å¼å¼€å‘è®¡åˆ’)
3. [ç‰ˆæœ¬è¯¦ç»†è®¾è®¡](#3-ç‰ˆæœ¬è¯¦ç»†è®¾è®¡)
4. [è¯„ä¼°å¯¹æ¯”æ–¹æ³•](#4-è¯„ä¼°å¯¹æ¯”æ–¹æ³•)

---

## 1. T-RAG è¿è¡ŒæŒ‡å—

### 1.1 ç¯å¢ƒå‡†å¤‡

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate trag

# éªŒè¯ä¾èµ–
python -c "import torch; import transformers; import sentence_transformers; print('Environment OK')"
```

### 1.2 æ•°æ®å‡†å¤‡ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰

é€‰æ‹©æ•°æ®é›†ï¼š**SQA (Sequential Question Answering)**
- åŸå› ï¼šå¤šè·³é—®ç­”ï¼Œæœ€é€‚åˆæµ‹è¯• adaptive planning
- è§„æ¨¡ï¼šé€‚ä¸­ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£
- é—®é¢˜ç±»å‹ï¼šéœ€è¦å¤šæ­¥æ¨ç†

```bash
cd /Users/sunyifei/Documents/GitHub/T-RAG/src/table2graph

# å‡†å¤‡æ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
bash scripts/prepare_data.sh
# è¿™ä¼šä¸‹è½½å¹¶å¤„ç† MultiTableQA æ•°æ®é›†ï¼ŒåŒ…æ‹¬ SQA

# éªŒè¯æ•°æ®
ls -lh data/sqa/
# åº”è¯¥çœ‹åˆ°ï¼š
# - sqa_source_tables.jsonl (æºè¡¨æ ¼)
# - sqa_sub_tables.jsonl (åˆ†è§£åçš„å­è¡¨)
# - sqa_table_schema.jsonl (è¡¨æ ¼schema)
# - sqa_example_query.jsonl (ç¤ºä¾‹æŸ¥è¯¢)
# - sqa_test_100.jsonl (æµ‹è¯•é›†ï¼Œ100ä¸ªæ ·æœ¬)
```

### 1.3 å®Œæ•´è¿è¡Œæµç¨‹ï¼ˆåŸºçº¿ T-RAGï¼‰

#### æ­¥éª¤1ï¼šè¡¨æ ¼èšç±»ï¼ˆStage 1&2ï¼‰

```bash
cd /Users/sunyifei/Documents/GitHub/T-RAG/src/table2graph

# è¿è¡Œèšç±»
bash scripts/table_cluster_run.sh

# å‚æ•°è¯´æ˜ï¼ˆåœ¨ table_cluster_run.sh ä¸­é…ç½®ï¼‰ï¼š
# - dataset: sqa
# - n_clusters: 3ï¼ˆèšç±»æ•°ï¼‰
# - k: 50ï¼ˆæ¯èšç±»çš„å…¸å‹èŠ‚ç‚¹æ•°ï¼‰
# - embedding_method: contriever

# è¾“å‡ºï¼š
# data/sqa/sqa_clustered_tables_k50_n3_contriever.jsonl

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/sqa/sqa_cluster_k50_n3_contriever.log
```

#### æ­¥éª¤2ï¼šå­å›¾æ£€ç´¢ï¼ˆStage 3ï¼‰

```bash
cd /Users/sunyifei/Documents/GitHub/T-RAG/src/table2graph

# è¿è¡ŒPageRankæ£€ç´¢
python scripts/subgraph_retrieve_run.py

# å‚æ•°è¯´æ˜ï¼ˆåœ¨ subgraph_retrieve_run.py ä¸­é…ç½®ï¼‰ï¼š
# - DATASET: sqa
# - testing_num: 100ï¼ˆæµ‹è¯•æ ·æœ¬æ•°ï¼‰
# - top_k: 50ï¼ˆæœ€ç»ˆè¿”å›çš„è¡¨æ ¼æ•°ï¼‰
# - cluster_embedding_method: contriever
# - table_to_graph_embedding_method: sentencetransformer

# è¾“å‡ºï¼š
# data/sqa/sqa_retrieved_tables_schema_100_50_contriever.jsonl

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/sqa/sqa_subgraph_testingnum100_topK50_sentencetransformer.log
```

#### æ­¥éª¤3ï¼šLLM æ¨ç†ï¼ˆStage 4ï¼‰

**æ³¨æ„ï¼šéœ€è¦é…ç½® API Key**

```bash
cd /Users/sunyifei/Documents/GitHub/T-RAG/src/downstream_inference

# é…ç½® key.json
cat > key.json << EOF
{
    "openai": "YOUR_OPENAI_API_KEY",
    "claude": "YOUR_CLAUDE_API_KEY"
}
EOF

# è¿è¡Œæ¨ç†
python call_llm.py \
    --dataset sqa \
    --topk 50 \
    --mode API \
    --model gpt-4o-mini \
    --testing_num 100 \
    --embedding_method contriever

# è¾“å‡ºï¼š
# output/sqa/gpt-4o-mini/output_100_50.jsonl
```

#### æ­¥éª¤4ï¼šè¯„ä¼°

```bash
cd /Users/sunyifei/Documents/GitHub/T-RAG/src/downstream_inference

# è¿è¡Œè¯„ä¼°
python evaluation.py \
    --dataset sqa \
    --model gpt-4o-mini \
    --topk 50 \
    --testing_num 100

# è¾“å‡ºï¼š
# output/sqa/gpt-4o-mini/results_100_50.json
# åŒ…å«æŒ‡æ ‡ï¼šexact_match, f1_score

# æŸ¥çœ‹ç»“æœ
cat output/sqa/gpt-4o-mini/results_100_50.json
```

### 1.4 å¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼ˆæ¨èï¼‰

ä¸ºäº†å¿«é€ŸéªŒè¯ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå°è§„æ¨¡æµ‹è¯•ï¼š

```bash
# ä¿®æ”¹ testing_num ä¸º 10ï¼ˆåœ¨å„ä¸ªè„šæœ¬ä¸­ï¼‰
# è¿™æ ·å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆä¸€æ¬¡å®Œæ•´æµ‹è¯•
```

---

## 2. å¢é‡å¼å¼€å‘è®¡åˆ’

### æ ¸å¿ƒåŸåˆ™
âœ… æ¯ä¸ªç‰ˆæœ¬éƒ½æ˜¯**ç‹¬ç«‹å¯è¿è¡Œ**çš„
âœ… æ¯ä¸ªç‰ˆæœ¬éƒ½èƒ½**ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡**ï¼ˆEM/F1ï¼‰
âœ… æ–°åŠŸèƒ½éƒ½æœ‰**å¼€å…³æ§åˆ¶**ï¼Œé»˜è®¤å…³é—­
âœ… ä¿ç•™**åŸå§‹ T-RAG ä»£ç **ï¼Œé€šè¿‡æ–°æ–‡ä»¶æ‰©å±•

### ç‰ˆæœ¬è·¯çº¿å›¾

```
ç‰ˆæœ¬0ï¼ˆåŸºçº¿ï¼‰: åŸå§‹ T-RAG
    â†“
ç‰ˆæœ¬1: + æŸ¥è¯¢åˆ†è§£æ¨¡å—ï¼ˆå¯é€‰å¼€å…³ï¼š--use_decompositionï¼‰
    â†“
ç‰ˆæœ¬2: + äº‹å®æå–è¯„ä¼°ï¼ˆå¯é€‰å¼€å…³ï¼š--use_fact_extractionï¼‰
    â†“
ç‰ˆæœ¬3: + é‡è§„åˆ’èƒ½åŠ›ï¼ˆå¯é€‰å¼€å…³ï¼š--use_replanï¼‰
    â†“
ç‰ˆæœ¬4: + å®Œæ•´ orchestratorï¼ˆå¯é€‰å¼€å…³ï¼š--use_adaptive_ragï¼‰
    â†“
ç‰ˆæœ¬5: æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜ã€æ‰¹å¤„ç†ã€Promptè°ƒä¼˜ï¼‰
```

### ç‰ˆæœ¬å¯¹æ¯”çŸ©é˜µ

| ç‰ˆæœ¬ | æŸ¥è¯¢åˆ†è§£ | äº‹å®æå– | é‡è§„åˆ’ | è¿­ä»£å¾ªç¯ | é¢„æœŸEMæå‡ | å¼€å‘æ—¶é—´ |
|------|---------|---------|--------|----------|-----------|----------|
| V0   | âŒ      | âŒ      | âŒ     | âŒ       | åŸºçº¿      | 0å¤©      |
| V1   | âœ…      | âŒ      | âŒ     | âŒ       | 0-1%      | 1å¤©      |
| V2   | âœ…      | âœ…      | âŒ     | âŒ       | +1-2%     | 1å¤©      |
| V3   | âœ…      | âœ…      | âœ…     | âŒ       | +2-3%     | 1.5å¤©    |
| V4   | âœ…      | âœ…      | âœ…     | âœ…       | +3-5%     | 2å¤©      |
| V5   | âœ…      | âœ…      | âœ…     | âœ…       | +4-6%     | 1å¤©      |

---

## 3. ç‰ˆæœ¬è¯¦ç»†è®¾è®¡

### ç‰ˆæœ¬0ï¼šåŸºçº¿ T-RAGï¼ˆéªŒè¯ç¯å¢ƒï¼‰

**ç›®æ ‡**ï¼šå»ºç«‹å¯é çš„åŸºçº¿æŒ‡æ ‡

**ä»»åŠ¡æ¸…å•**ï¼š
- [x] è¿è¡Œå®Œæ•´çš„ T-RAG pipeline
- [x] è®°å½• baseline æŒ‡æ ‡ï¼ˆEM/F1ï¼‰
- [x] éªŒè¯æ‰€æœ‰è„šæœ¬å¯æ­£å¸¸è¿è¡Œ
- [x] åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆ100æ ·æœ¬ SQAï¼‰

**è¿è¡Œæ–¹æ³•**ï¼š
```bash
# æŒ‰ç…§ 1.3 èŠ‚çš„æ­¥éª¤è¿è¡Œ
# è®°å½•è¾“å‡ºåˆ° baseline_results.json
```

**é¢„æœŸè¾“å‡º**ï¼š
```json
{
  "dataset": "sqa",
  "model": "gpt-4o-mini",
  "testing_num": 100,
  "topk": 50,
  "exact_match": 0.XX,
  "f1_score": 0.XX,
  "avg_retrieval_time": XX.XX,
  "avg_inference_time": XX.XX
}
```

---

### ç‰ˆæœ¬1ï¼šæ·»åŠ æŸ¥è¯¢åˆ†è§£ï¼ˆå¯é€‰å¼€å…³ï¼‰

**ç›®æ ‡**ï¼šå®ç°æŸ¥è¯¢åˆ†è§£åŠŸèƒ½ï¼Œä½†ä¸æ”¹å˜æ¨ç†æµç¨‹

**æ–°å¢æ–‡ä»¶**ï¼š
```
src/downstream_inference/
â”œâ”€â”€ adaptive_modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ query_decomposer.py       # ä» REAP ç§»æ¤
â”‚   â””â”€â”€ prompts.py                 # è¡¨æ ¼ä¸“ç”¨ prompts
â””â”€â”€ call_llm_v1.py                 # æ‰©å±•ç‰ˆæœ¬ï¼ˆç»§æ‰¿ call_llm.pyï¼‰
```

**æ ¸å¿ƒæ”¹åŠ¨**ï¼š

**1. adaptive_modules/query_decomposer.py**
```python
"""
æŸ¥è¯¢åˆ†è§£æ¨¡å— - ä» REAP ç§»æ¤å¹¶é€‚é…è¡¨æ ¼åœºæ™¯
"""
import json
import re
from .prompts import SYSTEM_PROMPT_QUERY_ANALYSIS, USER_PROMPT_QUERY_ANALYSIS

def analyze_and_decompose_query(query: str, llm_call_func) -> dict:
    """
    å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºåŸå­çº§å­é—®é¢˜

    Args:
        query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
        llm_call_func: LLMè°ƒç”¨å‡½æ•°ï¼ˆä¼ å…¥ä»¥ä¿æŒå…¼å®¹æ€§ï¼‰

    Returns:
        {
            "user_goal": str,
            "requirements": [
                {
                    "requirement_id": "req1",
                    "question": "å­é—®é¢˜1",
                    "depends_on": null
                },
                {
                    "requirement_id": "req2",
                    "question": "å­é—®é¢˜2ï¼ˆå¯èƒ½åŒ…å«å ä½ç¬¦ï¼‰",
                    "depends_on": "req1"
                }
            ]
        }
    """
    system_prompt = SYSTEM_PROMPT_QUERY_ANALYSIS
    user_prompt = USER_PROMPT_QUERY_ANALYSIS.format(query=query)

    # è°ƒç”¨ LLM
    response = llm_call_func(system_prompt, user_prompt)

    # è§£æ JSON
    match = re.search(r'\{.*\}', response, re.DOTALL)
    if not match:
        # é™çº§ï¼šå¦‚æœåˆ†è§£å¤±è´¥ï¼Œè¿”å›å•ä¸€éœ€æ±‚
        return {
            "user_goal": query,
            "requirements": [
                {
                    "requirement_id": "req1",
                    "question": query,
                    "depends_on": None
                }
            ]
        }

    result = json.loads(match.group(0))

    # éªŒè¯ç»“æ„
    if "requirements" not in result:
        raise ValueError("Invalid decomposition result")

    return result


def is_multi_hop_query(decomposition: dict) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå¤šè·³æŸ¥è¯¢
    """
    return len(decomposition["requirements"]) > 1
```

**2. adaptive_modules/prompts.py**
```python
"""
è¡¨æ ¼ä¸“ç”¨ Prompt æ¨¡æ¿
"""

SYSTEM_PROMPT_QUERY_ANALYSIS = """
You are an expert in table-based question answering and query analysis. Your task is to analyze a user's question and break it down into atomic sub-questions (requirements) that can be answered by searching and analyzing tables.

**Key Guidelines for Table QA:**

1. **Understand Table Context**: Questions often require:
   - Finding specific tables by topic/caption
   - Locating specific columns in tables
   - Filtering rows based on conditions
   - Extracting cell values

2. **Decomposition Strategy**:
   - **Single-hop**: Question can be answered from one table lookup
     â†’ Return a single requirement
   - **Multi-hop**: Question needs multiple steps
     â†’ Break into sequential requirements with dependencies

3. **Requirement Format**:
   - Each requirement must be hyper-specific
   - Include all constraints from the original question
   - Use placeholders like [answer from req1] for dependent requirements

**Output Format**: JSON only, no extra text.

```json
{
  "user_goal": "<brief summary of what user wants>",
  "requirements": [
    {
      "requirement_id": "req1",
      "question": "<specific table lookup question>",
      "depends_on": null
    },
    {
      "requirement_id": "req2",
      "question": "<question using [answer from req1]>",
      "depends_on": "req1"
    }
  ]
}
```

**Examples**:

Example 1 (Single-hop):
Query: "What is the total revenue in 2023 from the financial report?"
Output:
```json
{
  "user_goal": "Find 2023 revenue from financial report",
  "requirements": [
    {
      "requirement_id": "req1",
      "question": "What is the total revenue in 2023 from financial report tables?",
      "depends_on": null
    }
  ]
}
```

Example 2 (Multi-hop):
Query: "What position was held by the actress in Kiss and Tell who was born in 1928?"
Output:
```json
{
  "user_goal": "Find government position of specific actress",
  "requirements": [
    {
      "requirement_id": "req1",
      "question": "Who was the actress in Kiss and Tell film who was born in 1928?",
      "depends_on": null
    },
    {
      "requirement_id": "req2",
      "question": "What government position was held by [answer from req1]?",
      "depends_on": "req1"
    }
  ]
}
```

CRITICAL: Output ONLY the JSON object, nothing else.
"""

USER_PROMPT_QUERY_ANALYSIS = """
User Question: {query}

Analyze and decompose this question into atomic requirements for table-based QA.
"""
```

**3. call_llm_v1.py** (åªæ˜¾ç¤ºå…³é”®ä¿®æ”¹éƒ¨åˆ†)
```python
"""
T-RAG with Query Decomposition (Version 1)
æ–°å¢å‚æ•°ï¼š--use_decomposition
"""
import argparse
from adaptive_modules.query_decomposer import analyze_and_decompose_query, is_multi_hop_query

# ... (ä¿ç•™åŸæœ‰çš„æ‰€æœ‰å‡½æ•°)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T-RAG with Adaptive Planning V1")
    # ... åŸæœ‰å‚æ•° ...
    parser.add_argument("--use_decomposition", action="store_true",
                        help="Enable query decomposition (V1 feature)")

    args = parser.parse_args()

    # ... åŸæœ‰ä»£ç  ...

    # V1 æ–°å¢é€»è¾‘
    decomposition_log = []  # è®°å½•åˆ†è§£ç»“æœç”¨äºåˆ†æ

    for i, line in enumerate(tqdm(retrieve_instances)):
        retrieve_instance = json.loads(line)
        query = retrieve_instance["query"]

        # V1: å¯é€‰çš„æŸ¥è¯¢åˆ†è§£
        if args.use_decomposition:
            try:
                decomposition = analyze_and_decompose_query(
                    query,
                    llm_call_func=lambda sys, usr: call_openai_api(sys, usr, model)
                )

                # è®°å½•åˆ†è§£ç»“æœ
                decomposition_log.append({
                    "query": query,
                    "decomposition": decomposition,
                    "is_multi_hop": is_multi_hop_query(decomposition)
                })

                # V1 é˜¶æ®µï¼šä»…åˆ†è§£ï¼Œä¸æ”¹å˜æ¨ç†æµç¨‹
                # ä»ç„¶ä½¿ç”¨åŸå§‹ query è¿›è¡Œæ¨ç†
                print(f"[V1] Decomposed into {len(decomposition['requirements'])} requirements")

            except Exception as e:
                print(f"[V1] Decomposition failed: {e}, falling back to original query")

        # åç»­æ¨ç†é€»è¾‘ä¿æŒä¸å˜
        # ... (åŸæœ‰çš„ prompt æ„å»ºå’Œ LLM è°ƒç”¨) ...

    # ä¿å­˜åˆ†è§£æ—¥å¿—
    if args.use_decomposition:
        decomposition_log_file = f"{output_dir}/decomposition_log_{testing_num}_{topk}.jsonl"
        with open(decomposition_log_file, "w") as f:
            for entry in decomposition_log:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        print(f"[V1] Decomposition log saved to {decomposition_log_file}")
```

**è¿è¡Œæ–¹æ³•**ï¼š
```bash
cd /Users/sunyifei/Documents/GitHub/T-RAG/src/downstream_inference

# å¯¹æ¯”å®éªŒ1ï¼šä¸ä½¿ç”¨åˆ†è§£ï¼ˆåŸºçº¿ï¼‰
python call_llm_v1.py \
    --dataset sqa \
    --topk 50 \
    --mode API \
    --model gpt-4o-mini \
    --testing_num 100 \
    --embedding_method contriever

# å¯¹æ¯”å®éªŒ2ï¼šä½¿ç”¨åˆ†è§£
python call_llm_v1.py \
    --dataset sqa \
    --topk 50 \
    --mode API \
    --model gpt-4o-mini \
    --testing_num 100 \
    --embedding_method contriever \
    --use_decomposition

# è¯„ä¼°ï¼ˆä¸¤æ¬¡å®éªŒä½¿ç”¨ç›¸åŒçš„è¯„ä¼°è„šæœ¬ï¼‰
python evaluation.py --dataset sqa --model gpt-4o-mini --topk 50 --testing_num 100
```

**è¯„ä¼°æ–¹æ³•**ï¼š
```bash
# å¯¹æ¯”ä¸¤æ¬¡è¿è¡Œçš„ç»“æœ
# V1 é˜¶æ®µé¢„æœŸï¼šEM/F1 åŸºæœ¬ç›¸åŒï¼ˆå› ä¸ºæ²¡æœ‰æ”¹å˜æ¨ç†æµç¨‹ï¼‰
# ä½†æ˜¯å¯ä»¥ä» decomposition_log ä¸­åˆ†æï¼š
# - æœ‰å¤šå°‘æŸ¥è¯¢è¢«è¯†åˆ«ä¸º multi-hop
# - åˆ†è§£è´¨é‡å¦‚ä½•
```

---

### ç‰ˆæœ¬2ï¼šæ·»åŠ äº‹å®æå–è¯„ä¼°

**ç›®æ ‡**ï¼šåœ¨åˆ†è§£åçš„æ¯ä¸ªå­é—®é¢˜ä¸Šè¯„ä¼°äº‹å®æå–è´¨é‡

**æ–°å¢æ–‡ä»¶**ï¼š
```
src/downstream_inference/adaptive_modules/
â”œâ”€â”€ fact_extractor.py              # æ–°å¢
â””â”€â”€ call_llm_v2.py                 # æ–°ç‰ˆæœ¬
```

**æ ¸å¿ƒæ”¹åŠ¨**ï¼š

**1. adaptive_modules/fact_extractor.py**
```python
"""
äº‹å®æå–æ¨¡å— - è¯„ä¼°ä»è¡¨æ ¼ä¸­æå–äº‹å®çš„è´¨é‡
"""
import json
import re

# ä» REAP prompts ç§»æ¤å¹¶ä¿®æ”¹
SYSTEM_PROMPT_FACT_EXTRACTION = """
You are a table data extraction expert. Your task is to extract facts from retrieved tables that answer a specific requirement.

**Input**:
1. A specific requirement (sub-question)
2. Retrieved tables (with caption, headers, and rows)
3. Previously collected facts (context)

**Your Task**:
1. Read the tables carefully
2. Identify the relevant columns and rows
3. Extract the precise fact that answers the requirement
4. Classify the extraction quality

**Extraction Quality Levels**:
- **DIRECT_ANSWER**: Found a clear, direct answer in the tables
- **PARTIAL_CLUE**: Found partial information, but not complete
- **FAILED_EXTRACT**: No relevant information in the tables

**Output Format** (JSON only):
```json
{
  "reasoned_facts": [
    {
      "fulfills_requirement_id": "req1",
      "reasoning": "<explain how you found the answer in the table>",
      "statement": "<the extracted fact>",
      "fulfillment_level": "DIRECT_ANSWER|PARTIAL_CLUE|FAILED_EXTRACT"
    }
  ]
}
```

**Example**:
Requirement: "What is the total revenue in 2023?"
Table: Financial Report with columns [Year, Revenue, Profit]
Row: [2023, $1.2B, $200M]

Output:
```json
{
  "reasoned_facts": [
    {
      "fulfills_requirement_id": "req1",
      "reasoning": "Found in Financial Report table, column 'Revenue', row where Year=2023",
      "statement": "The total revenue in 2023 was $1.2B",
      "fulfillment_level": "DIRECT_ANSWER"
    }
  ]
}
```

CRITICAL: Output ONLY valid JSON.
"""

def extract_fact_from_tables(
    requirement: dict,
    retrieved_tables: list,
    collected_facts: list,
    llm_call_func
) -> dict:
    """
    ä»æ£€ç´¢åˆ°çš„è¡¨æ ¼ä¸­æå–äº‹å®

    Args:
        requirement: {"requirement_id": "req1", "question": "..."}
        retrieved_tables: æ£€ç´¢åˆ°çš„è¡¨æ ¼åˆ—è¡¨
        collected_facts: å·²æ”¶é›†çš„äº‹å®ï¼ˆæä¾›ä¸Šä¸‹æ–‡ï¼‰
        llm_call_func: LLMè°ƒç”¨å‡½æ•°

    Returns:
        {
            "reasoned_facts": [
                {
                    "fulfills_requirement_id": "req1",
                    "reasoning": "...",
                    "statement": "...",
                    "fulfillment_level": "DIRECT_ANSWER"
                }
            ]
        }
    """
    # æ„å»º prompt
    system_prompt = SYSTEM_PROMPT_FACT_EXTRACTION

    # æ ¼å¼åŒ–è¡¨æ ¼
    tables_text = ""
    for i, table in enumerate(retrieved_tables[:10]):  # é™åˆ¶è¡¨æ ¼æ•°é‡
        tables_text += f"\nTable {i+1}:\n"
        tables_text += f"Caption: {table.get('caption', 'N/A')}\n"
        headers = table.get('table', {}).get('header', [])
        rows = table.get('table', {}).get('rows', [])
        tables_text += f"Headers: {' | '.join(headers)}\n"
        tables_text += f"Rows (showing first 5):\n"
        for row in rows[:5]:
            tables_text += f"  {' | '.join(row)}\n"

    # æ ¼å¼åŒ–å·²çŸ¥äº‹å®
    facts_text = ""
    if collected_facts:
        facts_text = "\nPreviously collected facts:\n"
        for fact in collected_facts:
            facts_text += f"- {fact.get('statement', '')}\n"

    user_prompt = f"""
Requirement ID: {requirement['requirement_id']}
Requirement Question: {requirement['question']}

Retrieved Tables:
{tables_text}
{facts_text}

Extract the fact that answers this requirement.
"""

    # è°ƒç”¨ LLM
    response = llm_call_func(system_prompt, user_prompt)

    # è§£æ JSON
    match = re.search(r'\{.*\}', response, re.DOTALL)
    if not match:
        # é™çº§ï¼šè¿”å›å¤±è´¥ç»“æœ
        return {
            "reasoned_facts": [
                {
                    "fulfills_requirement_id": requirement['requirement_id'],
                    "reasoning": "Failed to parse LLM response",
                    "statement": "N/A",
                    "fulfillment_level": "FAILED_EXTRACT"
                }
            ]
        }

    result = json.loads(match.group(0))
    return result
```

**2. call_llm_v2.py** (å…³é”®ä¿®æ”¹)
```python
"""
T-RAG with Fact Extraction (Version 2)
æ–°å¢å‚æ•°ï¼š--use_fact_extraction
"""
from adaptive_modules.fact_extractor import extract_fact_from_tables

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T-RAG with Adaptive Planning V2")
    # ... åŸæœ‰å‚æ•° ...
    parser.add_argument("--use_decomposition", action="store_true")
    parser.add_argument("--use_fact_extraction", action="store_true",
                        help="Enable fact extraction quality assessment (V2)")

    args = parser.parse_args()

    # ... åŸæœ‰ä»£ç  ...

    fact_extraction_log = []

    for i, line in enumerate(tqdm(retrieve_instances)):
        retrieve_instance = json.loads(line)
        query = retrieve_instance["query"]
        retrieved_tables = retrieve_instance["retrieved_tables"]

        # V1: æŸ¥è¯¢åˆ†è§£
        decomposition = None
        if args.use_decomposition:
            decomposition = analyze_and_decompose_query(query, llm_call_func)

        # V2: äº‹å®æå–è¯„ä¼°
        if args.use_fact_extraction and decomposition:
            collected_facts = []

            # ä¸ºæ¯ä¸ªéœ€æ±‚æå–äº‹å®
            for req in decomposition["requirements"]:
                extraction_result = extract_fact_from_tables(
                    requirement=req,
                    retrieved_tables=retrieved_tables,
                    collected_facts=collected_facts,
                    llm_call_func=llm_call_func
                )

                # è®°å½•ç»“æœ
                fact_extraction_log.append({
                    "query": query,
                    "requirement": req,
                    "extraction": extraction_result
                })

                # æ”¶é›†æˆåŠŸæå–çš„äº‹å®
                for fact in extraction_result["reasoned_facts"]:
                    if fact["fulfillment_level"] != "FAILED_EXTRACT":
                        collected_facts.append(fact)

                print(f"[V2] Extracted fact for {req['requirement_id']}: "
                      f"{fact['fulfillment_level']}")

        # åç»­æ¨ç†é€»è¾‘ä¿æŒä¸å˜
        # ... (åŸæœ‰çš„å®Œæ•´è¡¨æ ¼æ¨ç†) ...

    # ä¿å­˜äº‹å®æå–æ—¥å¿—
    if args.use_fact_extraction:
        fact_log_file = f"{output_dir}/fact_extraction_log_{testing_num}_{topk}.jsonl"
        with open(fact_log_file, "w") as f:
            for entry in fact_extraction_log:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
```

**è¿è¡Œæ–¹æ³•**ï¼š
```bash
# å¯¹æ¯”å®éªŒï¼šV2 vs V1
python call_llm_v2.py \
    --dataset sqa \
    --topk 50 \
    --model gpt-4o-mini \
    --testing_num 100 \
    --use_decomposition \
    --use_fact_extraction
```

**è¯„ä¼°åˆ†æ**ï¼š
```python
# åˆ†æ fact_extraction_log.jsonl
import json

with open("fact_extraction_log_100_50.jsonl") as f:
    logs = [json.loads(line) for line in f]

# ç»Ÿè®¡æå–è´¨é‡
stats = {
    "DIRECT_ANSWER": 0,
    "PARTIAL_CLUE": 0,
    "FAILED_EXTRACT": 0
}

for log in logs:
    level = log["extraction"]["reasoned_facts"][0]["fulfillment_level"]
    stats[level] += 1

print("Fact Extraction Quality:")
print(f"  DIRECT_ANSWER: {stats['DIRECT_ANSWER']} ({stats['DIRECT_ANSWER']/len(logs)*100:.1f}%)")
print(f"  PARTIAL_CLUE: {stats['PARTIAL_CLUE']} ({stats['PARTIAL_CLUE']/len(logs)*100:.1f}%)")
print(f"  FAILED_EXTRACT: {stats['FAILED_EXTRACT']} ({stats['FAILED_EXTRACT']/len(logs)*100:.1f}%)")
```

---

### ç‰ˆæœ¬3ï¼šæ·»åŠ é‡è§„åˆ’èƒ½åŠ›

**ç›®æ ‡**ï¼šå½“äº‹å®æå–å¤±è´¥æ—¶ï¼Œè§¦å‘æŸ¥è¯¢æ”¹å†™å¹¶é‡æ–°æ£€ç´¢

**æ–°å¢æ–‡ä»¶**ï¼š
```
src/downstream_inference/adaptive_modules/
â”œâ”€â”€ replanner.py                   # æ–°å¢
â””â”€â”€ call_llm_v3.py                 # æ–°ç‰ˆæœ¬
```

**æ ¸å¿ƒæ”¹åŠ¨**ï¼š

**1. adaptive_modules/replanner.py**
```python
"""
é‡è§„åˆ’æ¨¡å— - å½“æ£€ç´¢å¤±è´¥æ—¶æ”¹å†™æŸ¥è¯¢
"""
import json
import re

SYSTEM_PROMPT_REPLAN_LITE = """
You are a table search query optimization expert. When a search query fails to retrieve relevant tables or extract facts, you need to reformulate it.

**Your Task**:
Given:
1. Original query that failed
2. Why it failed (e.g., PARTIAL_CLUE or FAILED_EXTRACT)
3. Retrieved tables (which were not helpful)

Generate:
- A reformulated query that is more likely to succeed

**Reformulation Strategies**:
1. **Be more specific**: Add constraints, column names, or table types
2. **Change keywords**: Use synonyms or alternative phrasings
3. **Simplify**: If query was too complex, break it down further
4. **Add context**: Include domain-specific terms

**Output Format** (JSON only):
```json
{
  "diagnosis": "<why the original query failed>",
  "reformulated_query": "<the new query to try>",
  "strategy": "<which strategy you used>"
}
```

Example:
Original: "What is the revenue?"
Failed because: Too vague, many revenue tables
Reformulated: "What is the total revenue in 2023 from the annual financial report?"

CRITICAL: Output ONLY JSON.
"""

def replan_on_failure(
    original_query: str,
    failed_fact: dict,
    retrieved_tables: list,
    llm_call_func
) -> str:
    """
    å½“äº‹å®æå–å¤±è´¥æ—¶ï¼Œé‡æ–°è§„åˆ’æŸ¥è¯¢

    Args:
        original_query: åŸå§‹æŸ¥è¯¢
        failed_fact: å¤±è´¥çš„äº‹å®æå–ç»“æœ
        retrieved_tables: æ£€ç´¢åˆ°çš„è¡¨æ ¼ï¼ˆæœªæˆåŠŸï¼‰
        llm_call_func: LLMè°ƒç”¨å‡½æ•°

    Returns:
        reformulated_query: æ”¹å†™åçš„æŸ¥è¯¢
    """
    system_prompt = SYSTEM_PROMPT_REPLAN_LITE

    # æ„å»ºå¤±è´¥ä¿¡æ¯
    failure_info = f"""
Original Query: {original_query}
Failure Level: {failed_fact.get('fulfillment_level', 'UNKNOWN')}
Failure Reasoning: {failed_fact.get('reasoning', 'N/A')}

Retrieved Tables (captions):
"""
    for i, table in enumerate(retrieved_tables[:5]):
        failure_info += f"{i+1}. {table.get('caption', 'No caption')}\n"

    user_prompt = failure_info + "\n\nReformulate the query to improve retrieval."

    # è°ƒç”¨ LLM
    response = llm_call_func(system_prompt, user_prompt)

    # è§£æ JSON
    match = re.search(r'\{.*\}', response, re.DOTALL)
    if not match:
        # é™çº§ï¼šè¿”å›åŸæŸ¥è¯¢
        return original_query

    result = json.loads(match.group(0))
    return result.get("reformulated_query", original_query)
```

**2. call_llm_v3.py** (å…³é”®ä¿®æ”¹)
```python
"""
T-RAG with Replanning (Version 3)
æ–°å¢å‚æ•°ï¼š--use_replan
"""
from adaptive_modules.replanner import replan_on_failure

# éœ€è¦é›†æˆè¡¨æ ¼æ£€ç´¢æ¨¡å—
import sys
sys.path.append("../table2graph/subgraph_retrieve")
from subgraph_retrieve_sentencetransformer import retrieve_tables_for_query  # å°è£…åçš„æ¥å£

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T-RAG with Adaptive Planning V3")
    # ... åŸæœ‰å‚æ•° ...
    parser.add_argument("--use_replan", action="store_true",
                        help="Enable query replanning on extraction failure (V3)")
    parser.add_argument("--max_replan_attempts", type=int, default=2,
                        help="Maximum replanning attempts")

    args = parser.parse_args()

    # ... åŸæœ‰ä»£ç  ...

    replan_log = []

    for i, line in enumerate(tqdm(retrieve_instances)):
        retrieve_instance = json.loads(line)
        query = retrieve_instance["query"]
        retrieved_tables = retrieve_instance["retrieved_tables"]

        # V1: æŸ¥è¯¢åˆ†è§£
        decomposition = None
        if args.use_decomposition:
            decomposition = analyze_and_decompose_query(query, llm_call_func)

        # V2 & V3: äº‹å®æå– + é‡è§„åˆ’
        if args.use_fact_extraction and decomposition:
            collected_facts = []

            for req in decomposition["requirements"]:
                current_query = req["question"]
                current_tables = retrieved_tables
                attempts = 0

                while attempts < args.max_replan_attempts:
                    # æå–äº‹å®
                    extraction_result = extract_fact_from_tables(
                        requirement=req,
                        retrieved_tables=current_tables,
                        collected_facts=collected_facts,
                        llm_call_func=llm_call_func
                    )

                    fact = extraction_result["reasoned_facts"][0]

                    # V3: å¦‚æœæå–å¤±è´¥ä¸”å¯ç”¨é‡è§„åˆ’
                    if (fact["fulfillment_level"] in ["PARTIAL_CLUE", "FAILED_EXTRACT"]
                        and args.use_replan and attempts < args.max_replan_attempts - 1):

                        print(f"[V3] Extraction failed ({fact['fulfillment_level']}), "
                              f"replanning... (attempt {attempts + 1})")

                        # é‡æ–°è§„åˆ’æŸ¥è¯¢
                        reformulated_query = replan_on_failure(
                            original_query=current_query,
                            failed_fact=fact,
                            retrieved_tables=current_tables,
                            llm_call_func=llm_call_func
                        )

                        # è®°å½•é‡è§„åˆ’
                        replan_log.append({
                            "original_query": current_query,
                            "reformulated_query": reformulated_query,
                            "attempt": attempts + 1,
                            "reason": fact["fulfillment_level"]
                        })

                        # é‡æ–°æ£€ç´¢ï¼ˆæ³¨æ„ï¼šè¿™é‡Œéœ€è¦è°ƒç”¨ T-RAG çš„æ£€ç´¢æ¨¡å—ï¼‰
                        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨ç›¸åŒçš„è¡¨æ ¼é›†åˆä½†æ”¹å˜æŸ¥è¯¢
                        # å®Œæ•´ç‰ˆï¼šéœ€è¦é‡æ–°è¿è¡Œ PageRank
                        current_query = reformulated_query
                        # current_tables = retrieve_tables_for_query(reformulated_query, topk)

                        attempts += 1
                    else:
                        # æå–æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°
                        if fact["fulfillment_level"] != "FAILED_EXTRACT":
                            collected_facts.append(fact)
                        break

        # åç»­æ¨ç†é€»è¾‘ä¿æŒä¸å˜
        # ...

    # ä¿å­˜é‡è§„åˆ’æ—¥å¿—
    if args.use_replan:
        replan_log_file = f"{output_dir}/replan_log_{testing_num}_{topk}.jsonl"
        with open(replan_log_file, "w") as f:
            for entry in replan_log:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
```

**è¿è¡Œæ–¹æ³•**ï¼š
```bash
# V3 å®Œæ•´æµ‹è¯•
python call_llm_v3.py \
    --dataset sqa \
    --topk 50 \
    --model gpt-4o-mini \
    --testing_num 100 \
    --use_decomposition \
    --use_fact_extraction \
    --use_replan \
    --max_replan_attempts 2
```

**è¯„ä¼°åˆ†æ**ï¼š
```python
# åˆ†æé‡è§„åˆ’æ•ˆæœ
import json

with open("replan_log_100_50.jsonl") as f:
    replan_logs = [json.loads(line) for line in f]

print(f"Total replanning events: {len(replan_logs)}")
print(f"Queries that triggered replan: {len(set(log['original_query'] for log in replan_logs))}")

# å¯¹æ¯” V2 å’Œ V3 çš„ EM/F1
# é¢„æœŸï¼šV3 åœ¨å¤šè·³æŸ¥è¯¢ä¸Šæœ‰æå‡
```

---

### ç‰ˆæœ¬4ï¼šå®Œæ•´ Orchestrator

**ç›®æ ‡**ï¼šå®ç°å®Œæ•´çš„è¿­ä»£å¼æ£€ç´¢-æ¨ç†å¾ªç¯

**æ–°å¢æ–‡ä»¶**ï¼š
```
src/downstream_inference/adaptive_modules/
â”œâ”€â”€ orchestrator.py                # æ–°å¢
â””â”€â”€ call_adaptive_rag.py           # å…¨æ–°æ–‡ä»¶ï¼ˆç‹¬ç«‹äº call_llm.pyï¼‰
```

**æ ¸å¿ƒæ”¹åŠ¨**ï¼š

**1. adaptive_modules/orchestrator.py**
```python
"""
Adaptive RAG ç¼–æ’å™¨ - å®Œæ•´çš„è¿­ä»£å¾ªç¯
"""
import json
from .query_decomposer import analyze_and_decompose_query
from .fact_extractor import extract_fact_from_tables
from .replanner import replan_on_failure

class AdaptiveRAGOrchestrator:
    def __init__(
        self,
        llm_call_func,
        retrieve_func,
        max_iterations=3,
        enable_replan=True
    ):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨

        Args:
            llm_call_func: LLMè°ƒç”¨å‡½æ•°
            retrieve_func: è¡¨æ ¼æ£€ç´¢å‡½æ•° (query, topk) -> tables
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            enable_replan: æ˜¯å¦å¯ç”¨é‡è§„åˆ’
        """
        self.llm_call_func = llm_call_func
        self.retrieve_func = retrieve_func
        self.max_iterations = max_iterations
        self.enable_replan = enable_replan
        self.trace_log = []

    def run(self, query: str, topk: int = 50) -> dict:
        """
        è¿è¡Œå®Œæ•´çš„ Adaptive RAG æµç¨‹

        Returns:
            {
                "final_answer": str,
                "collected_facts": list,
                "iterations": int,
                "trace": list
            }
        """
        # Stage 1: æŸ¥è¯¢åˆ†è§£
        decomposition = analyze_and_decompose_query(query, self.llm_call_func)
        requirements = decomposition["requirements"]

        self.trace_log.append({
            "stage": "decomposition",
            "requirements": requirements
        })

        # Stage 2: æ„å»ºä¾èµ–å›¾å¹¶æ‹“æ‰‘æ’åº
        sorted_reqs = self._topological_sort(requirements)

        # Stage 3: è¿­ä»£æ‰§è¡Œ
        collected_facts = []
        iteration = 0

        for req in sorted_reqs:
            # ç”¨å·²çŸ¥äº‹å®æ›¿æ¢å ä½ç¬¦
            concrete_query = self._substitute_facts(req["question"], collected_facts)

            # æ£€ç´¢è¡¨æ ¼
            retrieved_tables = self.retrieve_func(concrete_query, topk)

            # æå–äº‹å®ï¼ˆæœ€å¤šå°è¯•2æ¬¡ï¼‰
            for attempt in range(2):
                extraction_result = extract_fact_from_tables(
                    requirement=req,
                    retrieved_tables=retrieved_tables,
                    collected_facts=collected_facts,
                    llm_call_func=self.llm_call_func
                )

                fact = extraction_result["reasoned_facts"][0]

                # å¦‚æœå¤±è´¥ä¸”å¯ç”¨é‡è§„åˆ’
                if (fact["fulfillment_level"] != "DIRECT_ANSWER"
                    and self.enable_replan and attempt == 0):

                    # é‡æ–°è§„åˆ’
                    reformulated_query = replan_on_failure(
                        original_query=concrete_query,
                        failed_fact=fact,
                        retrieved_tables=retrieved_tables,
                        llm_call_func=self.llm_call_func
                    )

                    # é‡æ–°æ£€ç´¢
                    concrete_query = reformulated_query
                    retrieved_tables = self.retrieve_func(reformulated_query, topk)

                    self.trace_log.append({
                        "stage": "replan",
                        "requirement_id": req["requirement_id"],
                        "original_query": req["question"],
                        "reformulated_query": reformulated_query
                    })
                else:
                    # æˆåŠŸæˆ–æ”¾å¼ƒ
                    break

            # æ”¶é›†äº‹å®
            if fact["fulfillment_level"] != "FAILED_EXTRACT":
                collected_facts.append(fact)

            self.trace_log.append({
                "stage": "extraction",
                "requirement_id": req["requirement_id"],
                "fact": fact
            })

            iteration += 1
            if iteration >= self.max_iterations:
                break

        # Stage 4: åˆæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = self._synthesize_answer(query, collected_facts)

        return {
            "final_answer": final_answer,
            "collected_facts": collected_facts,
            "iterations": iteration,
            "trace": self.trace_log
        }

    def _topological_sort(self, requirements):
        """æ‹“æ‰‘æ’åºï¼ˆå¤„ç†ä¾èµ–å…³ç³»ï¼‰"""
        # ç®€åŒ–å®ç°ï¼šå‡è®¾ä¾èµ–æ˜¯çº¿æ€§çš„
        sorted_reqs = []
        visited = set()

        def visit(req_id):
            if req_id in visited:
                return
            req = next(r for r in requirements if r["requirement_id"] == req_id)
            if req["depends_on"]:
                visit(req["depends_on"])
            sorted_reqs.append(req)
            visited.add(req_id)

        for req in requirements:
            visit(req["requirement_id"])

        return sorted_reqs

    def _substitute_facts(self, question, collected_facts):
        """ç”¨å·²çŸ¥äº‹å®æ›¿æ¢å ä½ç¬¦"""
        import re

        # æŸ¥æ‰¾å ä½ç¬¦ [answer from req1]
        pattern = r'\[answer from (req\d+)\]'
        matches = re.findall(pattern, question)

        for req_id in matches:
            # æ‰¾åˆ°å¯¹åº”çš„äº‹å®
            fact = next(
                (f for f in collected_facts
                 if f["fulfills_requirement_id"] == req_id),
                None
            )
            if fact:
                # æ›¿æ¢å ä½ç¬¦
                placeholder = f"[answer from {req_id}]"
                question = question.replace(placeholder, fact["statement"])

        return question

    def _synthesize_answer(self, query, collected_facts):
        """åˆæˆæœ€ç»ˆç­”æ¡ˆ"""
        # ç®€åŒ–ç‰ˆï¼šç›´æ¥ä½¿ç”¨æœ€åä¸€ä¸ªäº‹å®çš„ statement
        if collected_facts:
            return collected_facts[-1]["statement"]
        return "Unable to answer based on retrieved tables."
```

**2. call_adaptive_rag.py** (å…¨æ–°ç‹¬ç«‹æ–‡ä»¶)
```python
"""
T-RAG with Full Adaptive Planning (Version 4)
å®Œå…¨ç‹¬ç«‹çš„å®ç°ï¼Œä¸ä¿®æ”¹åŸæœ‰ call_llm.py
"""
import json
import argparse
from tqdm import tqdm
from adaptive_modules.orchestrator import AdaptiveRAGOrchestrator
import sys
sys.path.append("../table2graph/subgraph_retrieve")

def create_retrieve_function(dataset, cluster_method, topk):
    """
    åˆ›å»ºè¡¨æ ¼æ£€ç´¢å‡½æ•°çš„å·¥å‚
    """
    # è¿™é‡Œéœ€è¦å°è£… T-RAG çš„æ£€ç´¢é€»è¾‘
    # ç®€åŒ–ç‰ˆï¼šä»é¢„æ£€ç´¢çš„ç»“æœä¸­è¯»å–
    def retrieve_func(query, k):
        # TODO: è°ƒç”¨ T-RAG çš„å®é™…æ£€ç´¢æ¨¡å—
        # ç°åœ¨å…ˆè¿”å›é¢„æ£€ç´¢çš„ç»“æœ
        return []

    return retrieve_func

def main():
    parser = argparse.ArgumentParser(description="T-RAG with Full Adaptive Planning")
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--topk", type=int, default=50)
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--testing_num", type=int, required=True)
    parser.add_argument("--max_iterations", type=int, default=3)
    parser.add_argument("--enable_replan", action="store_true")

    args = parser.parse_args()

    # è¯»å–é¢„æ£€ç´¢çš„ç»“æœ
    retrieve_file = f"../table2graph/data/{args.dataset}/{args.dataset}_retrieved_tables_schema_{args.testing_num}_{args.topk}_contriever.jsonl"

    with open(retrieve_file) as f:
        instances = [json.loads(line) for line in f]

    # åˆ›å»º LLM è°ƒç”¨å‡½æ•°
    def llm_call_func(system_prompt, user_prompt):
        # TODO: è°ƒç”¨å®é™…çš„ LLM API
        return ""

    # åˆ›å»ºæ£€ç´¢å‡½æ•°
    retrieve_func = create_retrieve_function(args.dataset, "contriever", args.topk)

    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = AdaptiveRAGOrchestrator(
        llm_call_func=llm_call_func,
        retrieve_func=retrieve_func,
        max_iterations=args.max_iterations,
        enable_replan=args.enable_replan
    )

    # è¿è¡Œ
    results = []
    for instance in tqdm(instances):
        query = instance["query"]
        result = orchestrator.run(query, args.topk)

        results.append({
            "query": query,
            "final_answer": result["final_answer"],
            "ground_truth": instance["query_label"],
            "iterations": result["iterations"],
            "trace": result["trace"]
        })

    # ä¿å­˜ç»“æœ
    output_file = f"output/{args.dataset}/{args.model}/adaptive_rag_output_{args.testing_num}_{args.topk}.jsonl"
    with open(output_file, "w") as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + "\n")

    print(f"Results saved to {output_file}")

if __name__ == "__main__":
    main()
```

**è¿è¡Œæ–¹æ³•**ï¼š
```bash
# V4 å®Œæ•´ Adaptive RAG
python call_adaptive_rag.py \
    --dataset sqa \
    --topk 50 \
    --model gpt-4o-mini \
    --testing_num 100 \
    --max_iterations 3 \
    --enable_replan
```

---

## 4. è¯„ä¼°å¯¹æ¯”æ–¹æ³•

### 4.1 å»ºç«‹ Baseline

```bash
# è¿è¡ŒåŸå§‹ T-RAG
cd /Users/sunyifei/Documents/GitHub/T-RAG/src/downstream_inference

# æµ‹è¯•é›†ï¼š100ä¸ªæ ·æœ¬
python call_llm.py \
    --dataset sqa \
    --topk 50 \
    --model gpt-4o-mini \
    --testing_num 100 \
    --embedding_method contriever

python evaluation.py \
    --dataset sqa \
    --model gpt-4o-mini \
    --topk 50 \
    --testing_num 100

# è®°å½•åŸºçº¿æŒ‡æ ‡
cp output/sqa/gpt-4o-mini/results_100_50.json baseline_results.json
```

### 4.2 ç‰ˆæœ¬å¯¹æ¯”è¡¨

åˆ›å»ºå¯¹æ¯”è„šæœ¬ï¼š

```python
# compare_versions.py
import json
import pandas as pd

versions = [
    "baseline",
    "v1_decomposition",
    "v2_fact_extraction",
    "v3_replan",
    "v4_full_adaptive"
]

results = []

for version in versions:
    result_file = f"output/sqa/gpt-4o-mini/{version}_results_100_50.json"
    with open(result_file) as f:
        data = json.load(f)
        results.append({
            "Version": version,
            "EM": data["exact_match"],
            "F1": data["f1_score"],
            "Avg_Time": data.get("avg_inference_time", 0)
        })

df = pd.DataFrame(results)
print(df.to_markdown(index=False))

# è®¡ç®—æå‡
df["EM_Gain"] = df["EM"] - df.loc[0, "EM"]
df["F1_Gain"] = df["F1"] - df.loc[0, "F1"]
print("\nGains over baseline:")
print(df[["Version", "EM_Gain", "F1_Gain"]].to_markdown(index=False))
```

### 4.3 é”™è¯¯åˆ†æ

```python
# error_analysis.py
import json

def analyze_errors(output_file, ground_truth_file):
    """
    åˆ†æå“ªäº›é—®é¢˜å›ç­”é”™è¯¯ï¼Œä»¥åŠåŸå› 
    """
    with open(output_file) as f:
        outputs = [json.loads(line) for line in f]

    errors = []
    for output in outputs:
        if output["predicted"] != output["ground_truth"]:
            errors.append(output)

    print(f"Total errors: {len(errors)}")

    # åˆ†ç±»é”™è¯¯ç±»å‹
    error_types = {
        "retrieval_failure": 0,  # æ£€ç´¢å¤±è´¥
        "extraction_failure": 0,  # æå–å¤±è´¥
        "reasoning_failure": 0    # æ¨ç†å¤±è´¥
    }

    # éœ€è¦äººå·¥æ ‡æ³¨ä¸€éƒ¨åˆ†é”™è¯¯
    return errors

# ä½¿ç”¨
baseline_errors = analyze_errors("baseline_output.jsonl", "ground_truth.jsonl")
v4_errors = analyze_errors("v4_output.jsonl", "ground_truth.jsonl")

# å¯¹æ¯”ï¼šV4 ä¿®å¤äº†å“ªäº›é”™è¯¯ï¼Ÿå¼•å…¥äº†å“ªäº›æ–°é”™è¯¯?
fixed = set(baseline_errors) - set(v4_errors)
new_errors = set(v4_errors) - set(baseline_errors)

print(f"Fixed errors: {len(fixed)}")
print(f"New errors: {len(new_errors)}")
```

---

## 5. å®æ–½æ—¶é—´è¡¨

| å¤©æ•° | ä»»åŠ¡ | äº¤ä»˜ç‰© |
|------|------|--------|
| Day 0 | ç¯å¢ƒéªŒè¯ + åŸºçº¿æµ‹è¯• | baseline_results.json |
| Day 1 | å®ç° V1ï¼ˆæŸ¥è¯¢åˆ†è§£ï¼‰ | call_llm_v1.py + decomposition_log |
| Day 2 | å®ç° V2ï¼ˆäº‹å®æå–ï¼‰ | call_llm_v2.py + fact_extraction_log |
| Day 3 | å®ç° V3ï¼ˆé‡è§„åˆ’ï¼‰ | call_llm_v3.py + replan_log |
| Day 4-5 | å®ç° V4ï¼ˆå®Œæ•´ç¼–æ’å™¨ï¼‰ | call_adaptive_rag.py + orchestrator.py |
| Day 6 | æ€§èƒ½ä¼˜åŒ– + Promptè°ƒä¼˜ | æœ€ç»ˆç‰ˆæœ¬ |
| Day 7 | å®Œæ•´è¯„ä¼° + æŠ¥å‘Š | è¯„ä¼°æŠ¥å‘Š + å¯¹æ¯”è¡¨ |

---

## 6. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³å¼€å§‹**ï¼š
1. éªŒè¯ T-RAG ç¯å¢ƒå¹¶è¿è¡ŒåŸºçº¿ï¼ˆæŒ‰ç…§ 1.3 èŠ‚ï¼‰
2. è®°å½•åŸºçº¿æŒ‡æ ‡
3. æˆ‘å°†å¼€å§‹å®ç° V1ï¼ˆæŸ¥è¯¢åˆ†è§£æ¨¡å—ï¼‰

**éœ€è¦ä½ ç¡®è®¤çš„é—®é¢˜**ï¼š
1. æ˜¯å¦æœ‰ OpenAI API Keyï¼Ÿï¼ˆç”¨äºæµ‹è¯•ï¼‰
2. æµ‹è¯•é›†è§„æ¨¡ï¼š100ä¸ªæ ·æœ¬å¤Ÿå—ï¼Ÿè¿˜æ˜¯éœ€è¦æ›´å¤šï¼Ÿ
3. ä¼˜å…ˆçº§ï¼šæ˜¯å¦æŒ‰ç…§ V1 â†’ V2 â†’ V3 â†’ V4 çš„é¡ºåºï¼Ÿ
4. æ˜¯å¦éœ€è¦æˆ‘ç°åœ¨å°±å¼€å§‹å†™ä»£ç ï¼Ÿ

è¯·å‘Šè¯‰æˆ‘ä½ æƒ³ä»å“ªä¸€æ­¥å¼€å§‹ï¼
